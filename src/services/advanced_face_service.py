"""
åŸºäº InsightFace å’Œ DeepFace çš„å…ˆè¿›äººè„¸è¯†åˆ«æœåŠ¡
é‡‡ç”¨æœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œæä¾›æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ€§èƒ½
"""
import os
import cv2
import numpy as np
import logging
from typing import List, Tuple, Dict, Any, Optional, Union
from datetime import datetime
import sys
import base64

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.utils.enhanced_visualization import EnhancedFaceVisualizer
import pickle
import base64
from pathlib import Path

# å…ˆè¿›çš„äººè„¸è¯†åˆ«åº“
import insightface
from deepface import DeepFace
import onnxruntime

# æœ¬åœ°æ¨¡å—
from ..models.database import DatabaseManager, Person, FaceEncoding
from ..utils.config import config
from ..utils.model_manager import get_model_manager

logger = logging.getLogger(__name__)

class AdvancedFaceRecognitionService:
    """
    å…ˆè¿›çš„äººè„¸è¯†åˆ«æœåŠ¡
    
    ç‰¹æ€§:
    - ä½¿ç”¨ InsightFace è¿›è¡Œé«˜ç²¾åº¦äººè„¸æ£€æµ‹å’Œç‰¹å¾æå–
    - æ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹ (ArcFace, CosFace, SphereFace)
    - DeepFace ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼Œæ”¯æŒå¤šç§åç«¯
    - æ›´é«˜çš„è¯†åˆ«å‡†ç¡®ç‡ (99.83% on LFW)
    - æ›´å¿«çš„æ¨ç†é€Ÿåº¦
    - æ”¯æŒå¹´é¾„ã€æ€§åˆ«ã€æƒ…ç»ªç­‰å±æ€§åˆ†æ
    """
    
    def __init__(self, model_name: str = 'buffalo_l'):
        """
        åˆå§‹åŒ–å…ˆè¿›äººè„¸è¯†åˆ«æœåŠ¡
        
        Args:
            model_name: InsightFace æ¨¡å‹åç§°
                - buffalo_l: å¤§å‹æ¨¡å‹ï¼Œæœ€é«˜ç²¾åº¦
                - buffalo_m: ä¸­å‹æ¨¡å‹ï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
                - buffalo_s: å°å‹æ¨¡å‹ï¼Œæœ€å¿«é€Ÿåº¦
        """
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = get_model_manager()
        
        self.db_manager = DatabaseManager()
        self.model_name = model_name
        
        # åˆå§‹åŒ–å¢å¼ºå¯è§†åŒ–å™¨
        self.visualizer = EnhancedFaceVisualizer()
        
        # åˆå§‹åŒ– InsightFace
        self._init_insightface()
        
        # è®¾ç½® DeepFace é…ç½®
        self._init_deepface()
        
        # ä½¿ç”¨å†…å­˜ç¼“å­˜ç³»ç»Ÿ
        self._face_cache = {}
        self._load_face_cache()
        logger.info("ğŸ“ ä½¿ç”¨å†…å­˜ç¼“å­˜æ¨¡å¼")
        
        logger.info(f"å…ˆè¿›äººè„¸è¯†åˆ«æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
    
    def _init_insightface(self):
        """åˆå§‹åŒ– InsightFace"""
        try:
            # ä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨é…ç½® InsightFace è·¯å¾„
            model_root = self.model_manager.configure_insightface(self.model_name)
            
            # åˆå§‹åŒ–åº”ç”¨
            self.app = insightface.app.FaceAnalysis(
                name=self.model_name,
                root=model_root,
                providers=['CPUExecutionProvider']  # ä½¿ç”¨ CPUï¼ŒGPU å¯æ”¹ä¸º CUDAExecutionProvider
            )
            self.app.prepare(ctx_id=0, det_size=(640, 640))
            
            logger.info(f"InsightFace åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹è·¯å¾„: {model_root}")
            
        except Exception as e:
            logger.error(f"InsightFace åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.app = None
    
    def _init_deepface(self):
        """åˆå§‹åŒ– DeepFace é…ç½®"""
        # é…ç½® DeepFace æ¨¡å‹è·¯å¾„
        deepface_config = self.model_manager.configure_deepface()
        logger.info(f"DeepFace é…ç½®è·¯å¾„: {deepface_config['deepface_home']}")
        
        self.deepface_models = [
            'ArcFace',      # æœ€æ–°çš„ ArcFace æ¨¡å‹
            'Facenet512',   # é«˜ç»´ç‰¹å¾ FaceNet
            'VGG-Face',     # ç»å…¸ VGG-Face
            'OpenFace',     # è½»é‡çº§æ¨¡å‹
        ]
        self.current_deepface_model = 'ArcFace'
        
        logger.info("DeepFace é…ç½®å®Œæˆ")
    
    def detect_faces(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        é«˜ç²¾åº¦äººè„¸æ£€æµ‹
        
        Args:
            image: è¾“å…¥å›¾åƒ (BGR æ ¼å¼)
            
        Returns:
            æ£€æµ‹åˆ°çš„äººè„¸ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«ä½ç½®ã€å…³é”®ç‚¹ã€è´¨é‡è¯„åˆ†ç­‰
        """
        faces = []
        
        # è·å–äººè„¸æ£€æµ‹é˜ˆå€¼
        detection_threshold = getattr(config, 'DETECTION_THRESHOLD', 0.5)
        
        try:
            if self.app:
                # ä½¿ç”¨ InsightFace æ£€æµ‹
                results = self.app.get(image)
                
                for face in results:
                    # åº”ç”¨æ£€æµ‹é˜ˆå€¼è¿‡æ»¤
                    if face.det_score < detection_threshold:
                        logger.debug(f"äººè„¸æ£€æµ‹ç½®ä¿¡åº¦è¿‡ä½: {face.det_score:.3f} < {detection_threshold}")
                        continue
                        
                    face_info = {
                        'bbox': face.bbox.astype(int).tolist(),  # [x1, y1, x2, y2]
                        'landmarks': face.kps.astype(int).tolist(),  # 5ä¸ªå…³é”®ç‚¹
                        'det_score': float(face.det_score),  # æ£€æµ‹ç½®ä¿¡åº¦
                        'embedding': face.embedding,  # 512ç»´ç‰¹å¾å‘é‡
                        'age': getattr(face, 'age', None),
                        'gender': getattr(face, 'gender', None),
                        'quality': self._calculate_face_quality(face)
                    }
                    faces.append(face_info)
            
            else:
                # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ OpenCV æ£€æµ‹
                faces = self._detect_faces_opencv(image)
            
            logger.info(f"æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")
            return faces
            
        except Exception as e:
            logger.error(f"äººè„¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return []
    
    def _detect_faces_opencv(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ OpenCV è¿›è¡Œäººè„¸æ£€æµ‹ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
        try:
            # åŠ è½½ Haar çº§è”åˆ†ç±»å™¨
            cascade_path = os.path.join(cv2.__path__[0], 'data', 'haarcascade_frontalface_default.xml')
            if not os.path.exists(cascade_path):
                # ä½¿ç”¨é»˜è®¤è·¯å¾„
                cascade_path = 'haarcascade_frontalface_default.xml'
            
            face_cascade = cv2.CascadeClassifier(cascade_path)
            
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faces_cv = face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
            )
        except Exception as e:
            logger.warning(f"OpenCVäººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return []
        
        faces = []
        for (x, y, w, h) in faces_cv:
            face_info = {
                'bbox': [x, y, x+w, y+h],
                'landmarks': None,
                'det_score': 0.8,  # å‡è®¾çš„ç½®ä¿¡åº¦
                'embedding': None,
                'age': None,
                'gender': None,
                'quality': 0.7
            }
            faces.append(face_info)
        
        return faces
    
    def _calculate_face_quality(self, face) -> float:
        """è®¡ç®—äººè„¸è´¨é‡è¯„åˆ†"""
        quality_score = 1.0
        
        # åŸºäºæ£€æµ‹ç½®ä¿¡åº¦
        quality_score *= face.det_score
        
        # åŸºäºäººè„¸å¤§å°ï¼ˆé¢ç§¯ï¼‰
        bbox = face.bbox
        face_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
        if face_area < 2500:  # 50x50 åƒç´ 
            quality_score *= 0.5
        elif face_area < 10000:  # 100x100 åƒç´ 
            quality_score *= 0.8
        
        return float(quality_score)
    
    def extract_features(self, image: np.ndarray, face_info: Dict[str, Any]) -> Optional[np.ndarray]:
        """
        æå–äººè„¸ç‰¹å¾å‘é‡
        
        Args:
            image: åŸå§‹å›¾åƒ
            face_info: äººè„¸ä¿¡æ¯ï¼ˆåŒ…å«è¾¹ç•Œæ¡†ï¼‰
            
        Returns:
            512ç»´ç‰¹å¾å‘é‡ï¼Œå¦‚æœæå–å¤±è´¥è¿”å› None
        """
        try:
            if face_info.get('embedding') is not None:
                # å¦‚æœå·²ç»æœ‰ç‰¹å¾å‘é‡ï¼Œç›´æ¥è¿”å›
                return face_info['embedding']
            
            # è£å‰ªäººè„¸åŒºåŸŸ
            bbox = face_info['bbox']
            face_crop = image[bbox[1]:bbox[3], bbox[0]:bbox[2]]
            
            if face_crop.size == 0:
                return None
            
            # ä½¿ç”¨ DeepFace æå–ç‰¹å¾
            try:
                embedding = DeepFace.represent(
                    img_path=face_crop,
                    model_name=self.current_deepface_model,
                    enforce_detection=False
                )[0]['embedding']
                
                return np.array(embedding, dtype=np.float32)
                
            except Exception as e:
                logger.warning(f"DeepFace ç‰¹å¾æå–å¤±è´¥: {str(e)}")
                return None
            
        except Exception as e:
            logger.error(f"ç‰¹å¾æå–å¤±è´¥: {str(e)}")
            return None
    
    def enroll_person(self, name: str, image_path: str, description: Optional[str] = None, original_filename: Optional[str] = None) -> Dict[str, Any]:
        """
        é«˜ç²¾åº¦äººå‘˜å…¥åº“
        
        Args:
            name: äººå‘˜å§“å
            image_path: å›¾åƒè·¯å¾„ï¼ˆä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼‰
            description: äººå‘˜æè¿°
            original_filename: åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºæ•°æ®åº“å­˜å‚¨ï¼‰
            
        Returns:
            å…¥åº“ç»“æœä¿¡æ¯
        """
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                return {'success': False, 'error': 'æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶'}
            
            # æ£€æµ‹äººè„¸
            faces = self.detect_faces(image)
            
            if not faces:
                return {'success': False, 'error': 'æœªæ£€æµ‹åˆ°äººè„¸'}
            
            if len(faces) > 1:
                return {'success': False, 'error': 'æ£€æµ‹åˆ°å¤šä¸ªäººè„¸ï¼Œè¯·ä½¿ç”¨åªåŒ…å«ä¸€ä¸ªäººè„¸çš„å›¾åƒ'}
            
            face = faces[0]
            
            # è´¨é‡æ£€æŸ¥
            if face['quality'] < 0.5:
                return {'success': False, 'error': 'äººè„¸è´¨é‡ä¸è¶³ï¼Œè¯·ä½¿ç”¨æ›´æ¸…æ™°çš„å›¾åƒ'}
            
            # æå–ç‰¹å¾
            features = self.extract_features(image, face)
            if features is None:
                return {'success': False, 'error': 'ç‰¹å¾æå–å¤±è´¥'}
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼äººè„¸ï¼ˆåŒåå’Œä¸åŒåéƒ½æŸ¥é‡ï¼‰
            duplicate_threshold_value = config.get('face_recognition.duplicate_threshold', 0.95)
            if isinstance(duplicate_threshold_value, (int, float)):
                duplicate_threshold = float(duplicate_threshold_value)
            else:
                duplicate_threshold = 0.95  # é»˜è®¤å€¼

            similarity_threshold_percent = duplicate_threshold * 100

            # 1. æŸ¥é‡ï¼šåŒååŒè„¸ç¦æ­¢
            existing_person = self.db_manager.get_person_by_name(name)
            if existing_person:
                person_id_checked = getattr(existing_person, "id", None)
                if not isinstance(person_id_checked, int):
                    logger.error(f"existing_person.id ä¸æ˜¯intç±»å‹ï¼Œè·³è¿‡åŒåæŸ¥é‡ã€‚å®é™…ç±»å‹: {type(person_id_checked)}")
                else:
                    encodings = self.db_manager.get_face_encodings_by_person(person_id_checked)
                    for encoding_obj in encodings:
                        db_enc = encoding_obj.encoding
                        # åªå¯¹bytesç±»å‹åšååºåˆ—åŒ–
                        if isinstance(db_enc, bytes):
                            try:
                                db_feature = pickle.loads(db_enc)
                            except Exception as e:
                                logger.warning(f"ç‰¹å¾ååºåˆ—åŒ–å¤±è´¥: {e}")
                                continue
                            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                            similarity = float(np.dot(features, db_feature) / (np.linalg.norm(features) * np.linalg.norm(db_feature)))
                            match_score = similarity * 100
                            if match_score > similarity_threshold_percent:
                                return {
                                    'success': False,
                                    'error': f'è¯¥äººå‘˜å·²å­˜åœ¨ç›¸ä¼¼äººè„¸ (åŒ¹é…åº¦: {match_score:.1f}%ï¼Œé˜ˆå€¼: {similarity_threshold_percent:.1f}%)'
                                }
                for encoding_obj in encodings:
                    db_enc = encoding_obj.encoding
                    # åªå¯¹bytesç±»å‹åšååºåˆ—åŒ–
                    if isinstance(db_enc, bytes):
                        try:
                            db_feature = pickle.loads(db_enc)
                        except Exception as e:
                            logger.warning(f"ç‰¹å¾ååºåˆ—åŒ–å¤±è´¥: {e}")
                            continue
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        similarity = float(np.dot(features, db_feature) / (np.linalg.norm(features) * np.linalg.norm(db_feature)))
                        match_score = similarity * 100
                        if match_score > similarity_threshold_percent:
                            return {
                                'success': False,
                                'error': f'è¯¥äººå‘˜å·²å­˜åœ¨ç›¸ä¼¼äººè„¸ (åŒ¹é…åº¦: {match_score:.1f}%ï¼Œé˜ˆå€¼: {similarity_threshold_percent:.1f}%)'
                            }

            # 2. æŸ¥é‡ï¼šä¸åŒååŒè„¸ç¦æ­¢
            existing_match = self.recognize_face(image)
            if existing_match['matches']:
                best_match = existing_match['matches'][0]
                if best_match['name'] != name:
                    if best_match['match_score'] > similarity_threshold_percent:
                        return {
                            'success': False, 
                            'error': f'ç›¸ä¼¼äººè„¸å·²å­˜åœ¨ï¼š{best_match["name"]} (åŒ¹é…åº¦: {best_match["match_score"]:.1f}%ï¼Œé˜ˆå€¼: {similarity_threshold_percent:.1f}%)'
                        }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            try:
                # æ£€æŸ¥åŒåäººå‘˜æ˜¯å¦å·²å­˜åœ¨
                existing_person = self.db_manager.get_person_by_name(name)
                
                if existing_person:
                    # åŒåäººå‘˜å·²å­˜åœ¨ï¼Œä¸ºå…¶æ·»åŠ æ–°çš„äººè„¸ç‰¹å¾
                    person_id = getattr(existing_person, "id", None)
                    if not isinstance(person_id, int):
                        logger.error(f"existing_person.id ä¸æ˜¯intç±»å‹ï¼Œæ— æ³•å…¥åº“ã€‚å®é™…ç±»å‹: {type(person_id)}")
                        return {'success': False, 'error': 'æ•°æ®åº“äººå‘˜IDå¼‚å¸¸ï¼Œæ— æ³•å…¥åº“'}
                    logger.info(f"ä¸ºç°æœ‰äººå‘˜ {name} (ID: {person_id}) æ·»åŠ æ–°çš„äººè„¸ç‰¹å¾")
                else:
                    # åˆ›å»ºæ–°äººå‘˜è®°å½•
                    person = self.db_manager.create_person(name, description)
                    person_id = person.id
                    logger.info(f"åˆ›å»ºæ–°äººå‘˜: {name} (ID: {person_id})")
                
                # è¯»å–å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
                with open(image_path, 'rb') as f:
                    image_data = f.read()
                
                # ä¿å­˜ç‰¹å¾å‘é‡å’Œå›¾ç‰‡æ•°æ®
                bbox = face['bbox']
                face_bbox_str = f"[{int(bbox[0])},{int(bbox[1])},{int(bbox[2])},{int(bbox[3])}]"
                
                # ä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸ºimage_pathå­˜å‚¨ï¼Œè€Œä¸æ˜¯ä¸´æ—¶è·¯å¾„
                stored_image_path = original_filename if original_filename else os.path.basename(image_path)
                
                face_encoding = self.db_manager.add_face_encoding(
                    person_id=person_id,
                    encoding=features,
                    image_path=stored_image_path,  # å­˜å‚¨åŸå§‹æ–‡ä»¶å
                    image_data=image_data,
                    face_bbox=face_bbox_str,
                    confidence=face['quality'],
                    quality_score=face['quality']
                )
                
                # æ›´æ–°å†…å­˜ç¼“å­˜
                if person_id not in self._face_cache:
                    self._face_cache[person_id] = {
                        'name': name,
                        'embeddings': [],
                        'model': f"advanced_{self.model_name}"
                    }
                
                # å°†ç‰¹å¾å‘é‡å’Œäººè„¸IDä¸€èµ·æ·»åŠ åˆ°ç¼“å­˜
                self._face_cache[person_id]['embeddings'].append((features, face_encoding.id))
                
                logger.info(f"æˆåŠŸå…¥åº“äººè„¸ç‰¹å¾: {name} (äººå‘˜ID: {person_id}, ç‰¹å¾ID: {face_encoding.id})")
                
                return {
                    'success': True,
                    'person_id': person_id,
                    'face_encoding_id': face_encoding.id,
                    'quality_score': face['quality'],
                    'feature_dim': len(features),
                    'faces_detected': 1,
                    'face_encoding': features.tolist()  # å°†numpyæ•°ç»„è½¬æ¢ä¸ºPythonåˆ—è¡¨
                }
            except Exception as db_error:
                logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(db_error)}")
                return {'success': False, 'error': f'æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(db_error)}'}
        
        except Exception as e:
            logger.error(f"äººå‘˜å…¥åº“å¤±è´¥: {str(e)}")
            return {'success': False, 'error': f'å…¥åº“å¤±è´¥: {str(e)}'}
    
    def extract_face_embeddings(self, image: Union[np.ndarray, str]) -> Dict[str, Any]:
        """
        ä¸“é—¨ç”¨äºæå–äººè„¸ç‰¹å¾å‘é‡çš„æ–¹æ³•ï¼Œä¸è¿›è¡Œèº«ä»½è¯†åˆ«
        
        Args:
            image: å›¾åƒæ•°ç»„æˆ–å›¾åƒè·¯å¾„
            
        Returns:
            åŒ…å«äººè„¸ç‰¹å¾å‘é‡çš„ç»“æœ
        """
        try:
            # å¤„ç†è¾“å…¥å›¾åƒ
            if isinstance(image, str):
                img = cv2.imread(image)
                if img is None:
                    return {'success': False, 'error': 'æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶'}
            else:
                img = image.copy()
            
            if img is None:
                return {'success': False, 'error': 'æ— æ•ˆçš„å›¾åƒæ•°æ®'}
            
            # è·å–å›¾åƒå°ºå¯¸
            height, width = img.shape[:2]
            
            face_embeddings = []
            
            # ç›´æ¥ä½¿ç”¨InsightFaceè·å–äººè„¸å’Œç‰¹å¾
            try:
                if self.app is not None:
                    logger.info("å¼€å§‹ä½¿ç”¨InsightFaceè¿›è¡Œäººè„¸æ£€æµ‹å’Œç‰¹å¾æå–")
                    # ç›´æ¥è·å–æ‰€æœ‰äººè„¸å’Œç‰¹å¾
                    faces_with_features = self.app.get(img)
                    logger.info(f"InsightFaceæ£€æµ‹åˆ° {len(faces_with_features)} ä¸ªäººè„¸")
                    
                    for i, face_result in enumerate(faces_with_features):
                        logger.info(f"å¤„ç†ç¬¬ {i+1} ä¸ªäººè„¸")
                        # åº”ç”¨æ£€æµ‹é˜ˆå€¼è¿‡æ»¤
                        detection_threshold = getattr(config, 'DETECTION_THRESHOLD', 0.5)
                        logger.info(f"æ£€æµ‹ç½®ä¿¡åº¦: {face_result.det_score}, é˜ˆå€¼: {detection_threshold}")
                        
                        if face_result.det_score < detection_threshold:
                            logger.info(f"äººè„¸ {i+1} ç½®ä¿¡åº¦è¿‡ä½ï¼Œè·³è¿‡")
                            continue
                        
                        # æ„å»ºäººè„¸ä¿¡æ¯
                        try:
                            bbox = face_result.bbox.astype(int).tolist()
                            confidence = float(face_result.det_score)
                            embedding = face_result.normed_embedding.tolist()
                            
                            logger.info(f"äººè„¸ {i+1}: bbox={bbox}, confidence={confidence}, embedding_len={len(embedding)}")
                            
                            face_info = {
                                'bbox': bbox,
                                'confidence': confidence,
                                'quality': confidence,  # ä½¿ç”¨æ£€æµ‹ç½®ä¿¡åº¦ä½œä¸ºè´¨é‡åˆ†æ•°
                                'embedding': embedding  # ä½¿ç”¨æ ‡å‡†åŒ–çš„ç‰¹å¾å‘é‡
                            }
                            
                            face_embeddings.append(face_info)
                            logger.info(f"æˆåŠŸæ·»åŠ äººè„¸ {i+1} çš„ç‰¹å¾ä¿¡æ¯")
                            
                        except Exception as inner_e:
                            logger.error(f"æ„å»ºäººè„¸ä¿¡æ¯å¤±è´¥: {inner_e}")
                            import traceback
                            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                            continue
                        
                else:
                    return {'success': False, 'error': 'InsightFaceæ¨¡å‹æœªåˆå§‹åŒ–'}
            
            except Exception as e:
                logger.error(f"äººè„¸æ£€æµ‹å’Œç‰¹å¾æå–å¤±è´¥: {str(e)}")
                import traceback
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                return {'success': False, 'error': f'ç‰¹å¾æå–å¤±è´¥: {str(e)}'}
            
            return {
                'success': True,
                'faces': face_embeddings,
                'total_faces': len(face_embeddings),
                'model_info': f"InsightFace-{self.model_name}" if self.app else f"DeepFace-{self.current_deepface_model}",
                'image_size': [width, height]
            }
            
        except Exception as e:
            logger.error(f"äººè„¸ç‰¹å¾æå–å¤±è´¥: {str(e)}")
            return {'success': False, 'error': f'ç‰¹å¾æå–å¤±è´¥: {str(e)}'}
    
    def recognize_face(self, image: Union[np.ndarray, str]) -> Dict[str, Any]:
        """
        é«˜ç²¾åº¦äººè„¸è¯†åˆ«
        
        Args:
            image: å›¾åƒæ•°ç»„æˆ–å›¾åƒè·¯å¾„
            
        Returns:
            è¯†åˆ«ç»“æœï¼ŒåŒ…å«åŒ¹é…çš„äººå‘˜ä¿¡æ¯å’Œç½®ä¿¡åº¦
        """
        try:
            # å¤„ç†è¾“å…¥å›¾åƒ
            if isinstance(image, str):
                img = cv2.imread(image)
            else:
                img = image.copy()
            
            if img is None:
                return {'success': False, 'matches': [], 'error': 'æ— æ³•è¯»å–å›¾åƒ'}
            
            # æ£€æµ‹äººè„¸
            faces = self.detect_faces(img)
            
            if not faces:
                return {'success': True, 'matches': [], 'message': 'æœªæ£€æµ‹åˆ°äººè„¸'}
            
            all_matches = []
            
            for i, face in enumerate(faces):
                # æå–ç‰¹å¾
                features = self.extract_features(img, face)
                if features is None:
                    continue
                
                # ä¸æ•°æ®åº“ä¸­çš„ç‰¹å¾æ¯”è¾ƒ
                matches = self._match_features(features)
                
                # æ·»åŠ äººè„¸ä½ç½®ä¿¡æ¯
                for match in matches:
                    match['face_index'] = i
                    match['bbox'] = face['bbox']
                    match['quality'] = face['quality']
                
                all_matches.extend(matches)
            
            # æŒ‰åŒ¹é…åº¦æ’åº
            all_matches.sort(key=lambda x: x['match_score'], reverse=True)
            
            return {
                'success': True,
                'matches': all_matches,
                'total_faces': len(faces)
            }
        
        except Exception as e:
            logger.error(f"äººè„¸è¯†åˆ«å¤±è´¥: {str(e)}")
            return {'success': False, 'matches': [], 'error': f'è¯†åˆ«å¤±è´¥: {str(e)}'}
    
    def _match_features(self, features: np.ndarray) -> List[Dict[str, Any]]:
        """
        ç‰¹å¾åŒ¹é…
        
        Args:
            features: å¾…åŒ¹é…çš„ç‰¹å¾å‘é‡
            
        Returns:
            åŒ¹é…ç»“æœåˆ—è¡¨
        """
        matches = []
        # ä»é…ç½®æ–‡ä»¶è¯»å–å½“å‰çš„è¯†åˆ«é˜ˆå€¼
        import json
        try:
            with open('config.json', 'r') as f:
                config_data = json.load(f)
                threshold = config_data.get('face_recognition', {}).get('recognition_threshold', 0.3)
        except:
            threshold = 0.3  # é»˜è®¤å€¼
        
        try:
            # ä»å†…å­˜ç¼“å­˜è·å–æ•°æ®
            cache_items = self._face_cache.items()
            
            for person_id, cached_data in cache_items:
                cached_embeddings = cached_data['embeddings']
                
                # éå†è¯¥äººå‘˜çš„æ‰€æœ‰ç‰¹å¾å‘é‡
                for cached_features in cached_embeddings:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (èŒƒå›´: -1 åˆ° 1, è¶Šæ¥è¿‘1è¶Šç›¸ä¼¼)
                    similarity = self._cosine_similarity(features, cached_features)
                    
                    # è®¡ç®—æ¬§æ°è·ç¦» (èŒƒå›´: 0åˆ°æ— ç©·, è¶Šå°è¶Šç›¸ä¼¼)
                    distance = np.linalg.norm(features - cached_features)
                    
                    # å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºç™¾åˆ†æ¯”å½¢å¼çš„åŒ¹é…åº¦
                    # å¯¹äºäººè„¸ç‰¹å¾ï¼Œä½™å¼¦ç›¸ä¼¼åº¦é€šå¸¸åœ¨0.3-1.0ä¹‹é—´ï¼Œç›´æ¥è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                    match_score = max(0, similarity) * 100  # ç¡®ä¿éè´Ÿå¹¶è½¬æ¢ä¸º0-100%
                    
                    # ä½¿ç”¨ç›¸ä¼¼åº¦ä½œä¸ºåˆ¤æ–­æ ‡å‡†
                    if similarity > threshold:
                        matches.append({
                            'person_id': person_id,
                            'name': cached_data['name'],
                            'match_score': float(match_score),  # åŒ¹é…åº¦ç™¾åˆ†æ¯”
                            'distance': float(distance),        # æ¬§æ°è·ç¦»
                            'model': cached_data['model']
                        })
            
            # æŒ‰åŒ¹é…åº¦æ’åº
            matches.sort(key=lambda x: x['match_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"ç‰¹å¾åŒ¹é…å¤±è´¥: {str(e)}")
        
        return matches
    
    def _cosine_similarity(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–
        features1_norm = features1 / np.linalg.norm(features1)
        features2_norm = features2 / np.linalg.norm(features2)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = np.dot(features1_norm, features2_norm)
        
        return float(similarity)
    
    def _load_face_cache(self):
        """ä»æ•°æ®åº“åŠ è½½äººè„¸ç‰¹å¾ç¼“å­˜"""
        try:
            with self.db_manager.get_session() as session:
                # æŸ¥è¯¢æ‰€æœ‰ç¼–ç å’Œå¯¹åº”çš„äººå‘˜ä¿¡æ¯
                encodings = session.query(FaceEncoding, Person).join(Person, FaceEncoding.person_id == Person.id).all()
                
                for encoding, person in encodings:
                    features = encoding.get_encoding()
                    
                    if person.id not in self._face_cache:
                        self._face_cache[person.id] = {
                            'name': person.name,
                            'embeddings': [],  # å­˜å‚¨ (ç‰¹å¾å‘é‡, äººè„¸ID) å…ƒç»„
                            'model': 'advanced_buffalo_l'
                        }
                    
                    # å°†ç‰¹å¾å‘é‡å’Œå¯¹åº”çš„äººè„¸IDä¸€èµ·å­˜å‚¨
                    self._face_cache[person.id]['embeddings'].append((features, encoding.id))
                
                logger.info(f"åŠ è½½äº† {len(self._face_cache)} ä¸ªäººå‘˜çš„äººè„¸ç‰¹å¾ç¼“å­˜")
        
        except Exception as e:
            logger.error(f"åŠ è½½äººè„¸ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def analyze_face_attributes(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        åˆ†æäººè„¸å±æ€§ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€æƒ…ç»ªç­‰ï¼‰
        
        Args:
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            äººè„¸å±æ€§åˆ†æç»“æœ
        """
        try:
            results = []
            
            # æ£€æµ‹äººè„¸
            faces = self.detect_faces(image)
            
            for face in faces:
                bbox = face['bbox']
                face_crop = image[bbox[1]:bbox[3], bbox[0]:bbox[2]]
                
                if face_crop.size == 0:
                    continue
                
                try:
                    # ä½¿ç”¨ DeepFace åˆ†æå±æ€§
                    analysis = DeepFace.analyze(
                        img_path=face_crop,
                        actions=['age', 'gender', 'emotion', 'race'],
                        enforce_detection=False
                    )[0]
                    
                    attributes = {
                        'bbox': bbox,
                        'age': analysis.get('age'),
                        'gender': analysis.get('dominant_gender'),
                        'gender_confidence': analysis.get('gender', {}).get(analysis.get('dominant_gender', ''), 0),
                        'emotion': analysis.get('dominant_emotion'),
                        'emotion_scores': analysis.get('emotion', {}),
                        'race': analysis.get('dominant_race'),
                        'race_scores': analysis.get('race', {})
                    }
                    
                    results.append(attributes)
                    
                except Exception as e:
                    logger.warning(f"å±æ€§åˆ†æå¤±è´¥: {str(e)}")
                    continue
            
            return results
        
        except Exception as e:
            logger.error(f"äººè„¸å±æ€§åˆ†æå¤±è´¥: {str(e)}")
            return []
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        try:
            with self.db_manager.get_session() as session:
                total_persons = session.query(Person).count()
                total_encodings = session.query(FaceEncoding).count()
                
                # è®¡ç®—å¹³å‡æ¯äººç…§ç‰‡æ•°
                avg_photos_per_person = 0.0
                if total_persons > 0:
                    avg_photos_per_person = round(total_encodings / total_persons, 1)
                
                # è·å–æœ€è¿‘7å¤©çš„äººå‘˜ç»Ÿè®¡
                from datetime import timedelta
                recent_date = datetime.now() - timedelta(days=7)
                recent_persons = session.query(Person).filter(Person.created_at >= recent_date).count()
                
                return {
                    'total_persons': total_persons,
                    'total_encodings': total_encodings,
                    'avg_photos_per_person': avg_photos_per_person,
                    'recent_persons': recent_persons,
                    'cache_size': len(self._face_cache),
                    'current_model': f"InsightFace_{self.model_name}",
                    'deepface_model': self.current_deepface_model,
                    'supported_models': self.deepface_models,
                    'recognition_threshold': config.RECOGNITION_THRESHOLD,
                    'detection_threshold': getattr(config, 'DETECTION_THRESHOLD', 0.5),
                    'duplicate_threshold': config.get('face_recognition.duplicate_threshold', 0.95)
                }
        
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def recognize_face_with_threshold(self, image: np.ndarray, threshold: float = 0.25) -> Dict[str, Any]:
        """
        ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼è¿›è¡Œäººè„¸è¯†åˆ«
        
        Args:
            image: è¾“å…¥å›¾åƒ
            threshold: è¯†åˆ«é˜ˆå€¼
            
        Returns:
            è¯†åˆ«ç»“æœå­—å…¸
        """
        try:
            start_time = datetime.now()
            
            # æ£€æµ‹äººè„¸
            faces = self.detect_faces(image)
            logger.info(f"æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")
            
            if not faces:
                return {
                    'success': True,
                    'matches': [],
                    'total_faces': 0,
                    'message': 'æœªæ£€æµ‹åˆ°äººè„¸'
                }

            matches = []
            
            # å¯¹æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸è¿›è¡Œè¯†åˆ«
            for face in faces:
                bbox = face['bbox']
                face_embedding = face.get('embedding')
                
                if face_embedding is None:
                    continue
                
                # åœ¨å·²çŸ¥äººè„¸ä¸­æŸ¥æ‰¾åŒ¹é…
                best_match = None
                best_similarity = 0
                matched_face_encoding_id = None
                
                for person_id, cached_data in self._face_cache.items():
                    for cached_embedding, face_encoding_id in cached_data['embeddings']:
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        similarity = float(np.dot(face_embedding, cached_embedding) / 
                                         (np.linalg.norm(face_embedding) * np.linalg.norm(cached_embedding)))
                        
                        if similarity > best_similarity:
                            best_similarity = similarity
                            matched_face_encoding_id = face_encoding_id
                            best_match = {
                                'person_id': person_id,
                                'name': cached_data['name'],
                                'match_score': similarity * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                                'distance': 1 - similarity,
                                'model': f"InsightFace_{self.model_name}",
                                'bbox': bbox,
                                'quality': face.get('det_score', 0.9),
                                'face_encoding_id': face_encoding_id  # æ·»åŠ åŒ¹é…çš„äººè„¸ID
                            }
                
                # è®°å½•è°ƒè¯•ä¿¡æ¯
                logger.info(f"è¯†åˆ«ç»“æœ - æœ€ä½³ç›¸ä¼¼åº¦: {best_similarity:.3f}, é˜ˆå€¼: {threshold:.3f}")
                if best_match:
                    logger.info(f"æœ€ä½³åŒ¹é…: {best_match['name']}, ç›¸ä¼¼åº¦: {best_similarity:.3f}")
                
                # åªè¿”å›è¶…è¿‡é˜ˆå€¼çš„åŒ¹é…
                if best_match and best_similarity >= threshold:
                    logger.info(f"è¯†åˆ«æˆåŠŸ: {best_match['name']}, ç›¸ä¼¼åº¦: {best_similarity:.3f} >= é˜ˆå€¼: {threshold:.3f}")
                    matches.append(best_match)
                else:
                    # æ·»åŠ æœªè¯†åˆ«çš„äººè„¸ä¿¡æ¯
                    if best_match:
                        logger.info(f"è¯†åˆ«å¤±è´¥: æœ€ä½³åŒ¹é… {best_match['name']} ç›¸ä¼¼åº¦ {best_similarity:.3f} < é˜ˆå€¼ {threshold:.3f}")
                    else:
                        logger.info(f"è¯†åˆ«å¤±è´¥: æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„äººè„¸")
                    matches.append({
                        'person_id': -1,
                        'name': 'æœªçŸ¥äººå‘˜',
                        'match_score': (best_similarity if best_match else 0.0) * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                        'distance': 1 - (best_similarity if best_match else 0.0),
                        'model': f"InsightFace_{self.model_name}",
                        'bbox': bbox,
                        'quality': face.get('det_score', 0.9),
                        'face_encoding_id': matched_face_encoding_id if matched_face_encoding_id else None  # å³ä½¿æœªè¯†åˆ«ä¹Ÿè¿”å›æœ€ä½³åŒ¹é…çš„äººè„¸ID
                    })
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return {
                'success': True,
                'matches': matches,
                'total_faces': len(faces),
                'processing_time': processing_time,
                'threshold_used': threshold,
                'message': f'è¯†åˆ«å®Œæˆï¼Œæ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸ï¼Œè¯†åˆ«å‡º {len([m for m in matches if m["person_id"] != -1])} ä¸ªå·²çŸ¥äººå‘˜'
            }
            
        except Exception as e:
            logger.error(f"äººè„¸è¯†åˆ«å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'matches': [],
                'total_faces': 0,
                'error': str(e)
            }
    
    def visualize_face_detection(self, image_path: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆäººè„¸æ£€æµ‹å¯è§†åŒ–å›¾åƒï¼ˆä½¿ç”¨å¢å¼ºå¯è§†åŒ–å™¨ï¼‰
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: åŒ…å«å¯è§†åŒ–ç»“æœçš„å­—å…¸
        """
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                return {
                    'success': False,
                    'error': 'æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶'
                }
            
            # æ£€æµ‹äººè„¸
            faces_data = []
            if self.app:
                faces = self.app.get(image)
                for i, face in enumerate(faces):
                    bbox = face.bbox.astype(int)
                    face_info = {
                        'bbox': bbox.tolist(),
                        'quality': float(face.det_score),
                        'det_score': float(face.det_score),
                        'name': f'äººè„¸ {i+1}'
                    }
                    faces_data.append(face_info)
            
            # ä½¿ç”¨å¢å¼ºå¯è§†åŒ–å™¨ç”Ÿæˆå›¾åƒ
            result = self.visualizer.visualize_face_detection(image, faces_data)
            
            if result['success']:
                return {
                    'success': True,
                    'image_base64': result['image_base64'],
                    'faces': result['face_details'],
                    'total_faces': result['total_faces'],
                    'message': f'æ£€æµ‹åˆ° {result["total_faces"]} ä¸ªäººè„¸'
                }
            else:
                return result
            
        except Exception as e:
            logger.error(f"äººè„¸æ£€æµ‹å¯è§†åŒ–å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }

# å…¨å±€æœåŠ¡å®ä¾‹
advanced_face_service = None

def get_advanced_face_service() -> AdvancedFaceRecognitionService:
    """è·å–å…ˆè¿›äººè„¸è¯†åˆ«æœåŠ¡å®ä¾‹"""
    global advanced_face_service
    if advanced_face_service is None:
        advanced_face_service = AdvancedFaceRecognitionService()
    return advanced_face_service
